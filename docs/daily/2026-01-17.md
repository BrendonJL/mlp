# 2026-01-17 - Phase 5: PPO v4 Evaluation & Documentation

## Session Focus
Evaluated the completed PPO v4 10M step training run with frame skip, updated analysis notebook, and prepared documentation for the breakthrough results.

## Major Accomplishments

### 1. PPO v4 Evaluation - BREAKTHROUGH CONFIRMED! üöÄ

The 10M step training run with frame skip (skip=4) completed overnight and the results are spectacular:

**Evaluation Results (10 episodes):**
| Metric | Value |
|--------|-------|
| Avg Distance | **2,725 pixels** |
| Max Distance | 2,757 pixels |
| Min Distance | 2,470 pixels |
| Avg Reward | **6,210** |
| Success Rate | 0% (flag at 3,266) |

**Comparison to Previous Agents:**
| Agent | Avg Distance | Avg Reward | vs Random |
|-------|-------------|------------|-----------|
| Random | 350 px | 380 | 1.0x |
| DQN (2M) | 1,024 px | 1,920 | 2.9x |
| PPO v3 (10M) | 1,319 px | 2,025 | 3.8x |
| **PPO v4 (10M+Skip)** | **2,725 px** | **6,210** | **7.8x** üèÜ |

**Key Achievement:** Agent now reaches **83% of the level** consistently (2,725/3,266 pixels)!

### 2. Updated Analysis Notebook

Completely updated `03_ppo_vs_dqn_comparison.ipynb` with PPO v4 data:

**New/Updated Cells:**
- Header with v4 experiment info
- Data loading for experiment_id 38 (18,985 episodes)
- Summary stats with all 5 agents
- Colorblind-friendly histogram (normalized to percent)
- PPO v4 individual learning curve
- Combined learning curve (dark theme, all agents overlaid)
- Evaluation bar chart comparing final policies
- Updated conclusions with v4 breakthrough

**Visualization Improvements:**
- Switched to colorblind-friendly palette (blue, orange, purple, teal)
- Normalized histogram to percentages for fair comparison across different episode counts
- Added sexy dark-themed combined learning curve

### 3. Condensed README.md

Significantly condensed README while keeping key elements:
- Results summary table at top
- All 4 agent videos (DQN, PPO failure, PPO v3, PPO v4)
- New v4 visualization images
- File tree structure
- Simplified tech stack table
- Project journey summary table
- Links to detailed documentation

Moved detailed phase descriptions, learning objectives, and daily highlights to ProjectDocumentation.md.

### 4. Evaluation Script Bug Fix

Found and documented a bug in `evaluate_model.py`:
- `--skip` argument was parsed but not passed to `create_eval_environment()`
- Fixed by adding `skip=args.skip` to the function call

## Technical Learnings

### Histogram Normalization for Fair Comparison
When comparing datasets with vastly different sizes (DQN: 785 eps vs PPO v4: 18,985 eps), raw counts make smaller datasets invisible. Solution: `histnorm='percent'` shows "what % of episodes ended at this distance" - fair comparison regardless of episode count.

### Colorblind-Friendly Visualization
Paul Tol palette for colorblind accessibility:
- Blue `#0077bb` - clearly distinct
- Orange `#ee7733` - warm tone
- Purple `#aa3377` - magenta family
- Teal `#009988` - cool tone

Avoids red/green combinations that cause issues for deuteranopia/protanopia.

### PPO v4 Episode Count Insight
PPO v4 ran **4x more episodes** than v3 (18,985 vs 4,684) in the same 10M steps because frame skip makes each episode faster - the agent covers more ground per decision, leading to quicker resolutions.

## Training Data Summary

**Database state after v4 completion:**
| Experiment | Episodes |
|------------|----------|
| random_baseline_world1-1 | 10 |
| dqn_baseline_world1-1 | 785 |
| ppo_v2_world1-1 | 2,197 |
| ppo_v3_world1-1 | 4,684 |
| ppo_v4_world1-1 | 18,985 |

## New Barrier Identified: ~2,700 Pixels

The agent consistently reaches 2,470-2,757 pixels but can't progress further. This tight range (287 pixel spread) indicates a reliable strategy up to a specific obstacle.

**Hypothesis:** There's likely a pit, enemy gauntlet, or platform sequence around x=2,700 that requires a skill the agent hasn't learned yet.

**Next Step:** PPO v5 with `SpeedrunRewardWrapper` removes base game rewards (coins/score) for cleaner progress-based signal. Larger milestone bonuses at 2,700+ may help push past this barrier.

## Files Modified

- `README.md` - Major condensation with v4 content
- `notebooks/03_ppo_vs_dqn_comparison.ipynb` - Full v4 update
- `scripts/evaluate_model.py` - Bug fix (skip parameter)
- `docs/ProjectDocumentation.md` - v4 results added
- `docs/daily/2026-01-17.md` - This file

## Files Created

- `docs/images/v4mlppics/` - New visualization images for README

## Next Steps

1. ‚è≥ Update ProjectDocumentation.md with v4 completion details
2. üöÄ Start PPO v5 training with SpeedrunRewardWrapper
3. üéØ Target: First level completion (reach flag at 3,266 pixels)!

---

**Training Status:** PPO v4 complete, preparing v5
**Next Session:** Launch PPO v5 with custom speedrun rewards
