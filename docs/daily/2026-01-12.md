# 2026-01-12 - Phase 5: PPO v2 Evaluation & Analysis

## Session Focus
Evaluated PPO v2 training results, created comparison notebook, and identified key learning challenges.

## Major Accomplishments

### 1. PPO v2 Training Results Analysis
Full 2M step training run completed overnight. Results:

| Metric | DQN (Exp 14) | PPO v2 (Exp 35) |
|--------|--------------|-----------------|
| Episodes | 785 | 2,197 |
| Avg Distance | 1,024 px | 687.7 px |
| Max Distance | 2,743 px | 2,226 px |
| Avg Reward | ~1,920 | ~700 |

**Key Finding:** DQN outperformed PPO despite PPO having reward shaping and 3x more episodes.

### 2. Created Comparison Notebook
Built `notebooks/03_ppo_vs_dqn_comparison.ipynb` with:
- Side-by-side learning curves
- Distance distribution box plots
- Histogram overlays
- Statistical analysis
- Performance percentiles

### 3. Database Cleanup
Removed all test runs, keeping only:
- Experiment 14: DQN baseline (785 episodes)
- Experiment 15: Random baseline (10 episodes)
- Experiment 35: PPO v2 (2,197 episodes)

### 4. Root Cause Analysis: Why PPO Underperformed

**The Tall Pipe Problem (x ≈ 700)**
- Agent consistently gets stuck at the first tall obstacle
- Can't figure out how to jump high enough
- With `deterministic=False`, matches training behavior (~700 distance)

**Why This Is Hard:**
1. **Jump height requires action chaining** - In Mario, holding jump longer = higher jump. With discrete actions, agent must learn to output multiple consecutive jump actions.
2. **Sparse success signal** - Random chance of clearing the pipe is very low
3. **PPO forgets rare successes** - On-policy learning discards experience after each update
4. **DQN's advantage** - Replay buffer remembers the rare times it randomly chained enough jumps

## Technical Learnings

### Deterministic vs Stochastic Evaluation
- `deterministic=True`: Always picks highest probability action (good for showcasing best behavior)
- `deterministic=False`: Samples from policy distribution (matches training behavior)
- For PPO v2, stochastic evaluation showed the "real" ~700 performance

### Action Space Limitations
`SIMPLE_MOVEMENT` has discrete actions - can't "hold" buttons. Options:
1. Use `COMPLEX_MOVEMENT` for more jump variations
2. Implement frame-skipping/sticky actions
3. Add reward shaping specifically for jump height

## Files Created/Modified

### New Files
- `notebooks/03_ppo_vs_dqn_comparison.ipynb` - Full analysis notebook

### Modified Files
- Database cleaned up (removed test experiments)

## Next Steps (Phase 5 Continued)
1. Add more granular milestones around x=700 obstacle (680, 720, 750)
2. Increase stuck timeout (150 → 300 steps) for more attempts
3. Consider `COMPLEX_MOVEMENT` action space
4. Plan 5M step training run with adjusted hyperparameters

## Key Insight
The fundamental challenge isn't the algorithm - it's that **Mario's physics require temporal action sequences** (holding jump) that discrete action spaces struggle to represent. The agent needs to discover that chaining jump actions creates higher jumps, which is a needle-in-a-haystack exploration problem.

---

**Training Status:** PPO v2 evaluation complete
**Next Session:** Hyperparameter tuning, 5M step run
