# 2026-01-12 - Phase 5: PPO v2 Evaluation & Analysis

## Session Focus
Evaluated PPO v2 training results, created comparison notebook, and identified key learning challenges.

## Major Accomplishments

### 1. PPO v2 Training Results Analysis
Full 2M step training run completed overnight. Results:

| Metric | DQN (Exp 14) | PPO v2 (Exp 35) |
|--------|--------------|-----------------|
| Episodes | 785 | 2,197 |
| Avg Distance | 1,024 px | 687.7 px |
| Max Distance | 2,743 px | 2,226 px |
| Avg Reward | ~1,920 | ~700 |

**Key Finding:** DQN outperformed PPO despite PPO having reward shaping and 3x more episodes.

### 2. Created Comparison Notebook
Built `notebooks/03_ppo_vs_dqn_comparison.ipynb` with:
- Side-by-side learning curves
- Distance distribution box plots
- Histogram overlays
- Statistical analysis
- Performance percentiles

### 3. Database Cleanup
Removed all test runs, keeping only:
- Experiment 14: DQN baseline (785 episodes)
- Experiment 15: Random baseline (10 episodes)
- Experiment 35: PPO v2 (2,197 episodes)

### 4. Root Cause Analysis: Why PPO Underperformed

**The Tall Pipe Problem (x ≈ 700)**
- Agent consistently gets stuck at the first tall obstacle
- Can't figure out how to jump high enough
- With `deterministic=False`, matches training behavior (~700 distance)

**Why This Is Hard:**
1. **Jump height requires action chaining** - In Mario, holding jump longer = higher jump. With discrete actions, agent must learn to output multiple consecutive jump actions.
2. **Sparse success signal** - Random chance of clearing the pipe is very low
3. **PPO forgets rare successes** - On-policy learning discards experience after each update
4. **DQN's advantage** - Replay buffer remembers the rare times it randomly chained enough jumps

## Technical Learnings

### Deterministic vs Stochastic Evaluation
- `deterministic=True`: Always picks highest probability action (good for showcasing best behavior)
- `deterministic=False`: Samples from policy distribution (matches training behavior)
- For PPO v2, stochastic evaluation showed the "real" ~700 performance

### Action Space Limitations
`SIMPLE_MOVEMENT` has discrete actions - can't "hold" buttons. Options:
1. Use `COMPLEX_MOVEMENT` for more jump variations
2. Implement frame-skipping/sticky actions
3. Add reward shaping specifically for jump height

## Files Created/Modified

### New Files
- `notebooks/03_ppo_vs_dqn_comparison.ipynb` - Full analysis notebook

### Modified Files
- Database cleaned up (removed test experiments)

## PPO v3 Configuration (Prepared for 10M Run)

Based on research of successful Mario PPO implementations, created `ppo_v3.yaml` with:

| Change | v2 Value | v3 Value | Rationale |
|--------|----------|----------|-----------|
| total_timesteps | 2M | **10M** | Successful runs trained 5x longer |
| use_lr_scheduler | false | **true** | Linear annealing improves stability |
| clip_range | 0.2 | **0.15** | Stanford's successful config |
| n_epochs | 5 | **10** | More data reuse per update |
| max_stuck_steps | 150 | **300** | More attempts at obstacles |

### Code Changes
- Added `linear_schedule()` function to `train.py` for LR annealing
- Updated `RewardShapingWrapper` stuck timeout (150 → 300)
- Training now prints LR scheduler status at startup

### Research Sources
- [yumouwei/super-mario-bros-rl](https://github.com/yumouwei/super-mario-bros-reinforcement-learning) - 10M steps, LR scheduler
- [Stanford CS224R Mario Project](https://cs224r.stanford.edu/projects/pdfs/CS224R_Project-1.pdf) - 96% success with clip=0.15
- [SB3 PPO Documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)

## Key Insight
The fundamental challenge isn't the algorithm - it's that **Mario's physics require temporal action sequences** (holding jump) that discrete action spaces struggle to represent. The agent needs to discover that chaining jump actions creates higher jumps, which is a needle-in-a-haystack exploration problem.

**Research revealed:** The biggest factor in successful runs was **training duration** (10M steps) and **LR scheduling**, not sticky actions.

---

**Training Status:** PPO v3 configured, ready for 10M step run (~20 hours)
**Next Session:** Evaluate v3 results
