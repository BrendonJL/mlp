---
id: "2025-12-31"
aliases: []
tags:
  - daily-notes
  - mario-rl-project
  - jupyter-notebooks
  - preprocessing
  - wrappers
  - phase-2-completion
created: 2025-12-31
---
# ðŸ“… Tuesday, December 31, 2025

> _Week 1 of 2026 - New Year's Eve!_

## ðŸŽ¯ Today's Goals

- [x] Implement random agent script
- [x] Create Jupyter notebook for environment exploration
- [x] Build complete frame preprocessing pipeline
- [x] **Complete majority of Phase 2 tasks**

## âœ… What I Accomplished

### Morning Session
- [x] Completed random agent implementation (`scripts/random_agent.py`) âœ… 2025-12-31
- [x] Debugged API compatibility issues (4-value vs 5-value unpacking) âœ… 2025-12-31
- [x] Added step limit (1000 steps/episode) to prevent infinite episodes âœ… 2025-12-31
- [x] Ran 10 episodes successfully with baseline performance (~380 avg reward) âœ… 2025-12-31
- [x] Set up Jupyter Lab and created `01_environment_exploration.ipynb` âœ… 2025-12-31
- [x] Visualized raw Mario frames (240Ã—256Ã—3 RGB) âœ… 2025-12-31
- [x] Created 6-frame sequence visualization âœ… 2025-12-31

### Afternoon Session
- [x] Built complete preprocessing pipeline in `src/environments/wrappers.py` âœ… 2025-12-31
- [x] **GrayscaleWrapper**: RGB (240Ã—256Ã—3) â†’ Grayscale (240Ã—256Ã—1) âœ… 2025-12-31
- [x] **ResizeWrapper**: (240Ã—256Ã—1) â†’ (84Ã—84Ã—1) using cv2 âœ… 2025-12-31
- [x] **NormalizeWrapper**: [0-255] â†’ [0-1] float32 âœ… 2025-12-31
- [x] **FrameStackWrapper**: (84Ã—84Ã—1) â†’ (84Ã—84Ã—4) with temporal stacking âœ… 2025-12-31
- [x] Installed opencv-python with NumPy 1.x compatibility âœ… 2025-12-31
- [x] Tested full wrapper chain successfully âœ… 2025-12-31

## ðŸ§  What I Learned

### Random Agent & RL Baselines

- **Random baseline importance**: Establishes minimum performance (~380 reward) that any trained agent must exceed
- **Episode step limits**: Prevent random agents from wandering indefinitely (set 1000 steps/episode)
- **Baseline variance**: Random performance ranged from 143-533 reward, showing high stochasticity
- **All episodes hit step limit**: Random Mario never died or won, just wandered until cutoff
- **Performance metric**: Reward correlates with distance traveled (forward movement = higher reward)

### Jupyter Notebooks for RL

- **Visual exploration value**: Seeing actual game frames makes preprocessing motivation clear
- **Notebook kernel management**: "Restart & Run All" fixes mysterious state issues
- **Cell execution order matters**: Variables only exist if cells complete successfully
- **Frame sequence visualization**: `plt.subplots()` creates grids to show temporal changes
- **Interactive debugging**: Notebooks excellent for iterative exploration before production code

### Gym Wrappers & Design Patterns

- **ObservationWrapper pattern**: Inherit from `gym.ObservationWrapper`, override `observation()` method
- **Wrapper composition**: Chain wrappers like middleware - each does ONE thing well
- **Decorator pattern in practice**: Wrappers add functionality without modifying base environment
- **Single Responsibility Principle**: Each wrapper has one job (grayscale, resize, normalize, stack)
- **API inheritance**: Wrappers inherit API quirks from wrapped envs (tuple returns, etc.)
- **Reset vs observation methods**: Some wrappers need to override `reset()` for initialization (FrameStackWrapper)

### Image Processing for RL

- **RGB to Grayscale conversion**: Weighted sum (RÃ—0.299 + GÃ—0.587 + BÃ—0.114) based on human perception
- **Why these weights?**: Eyes most sensitive to green, less to red, least to blue
- **cv2.resize() gotchas**: Takes (width, height) not (height, width); may squeeze channel dimension
- **Normalization benefits**: [0-1] range helps neural network training (better gradients)
- **dtype management**: Grayscale/resize use uint8, normalization converts to float32
- **Frame stacking rationale**: Single frames can't show motion/velocity - need temporal context

### Collections & Data Structures

- **deque with maxlen**: Perfect for sliding windows - auto-removes oldest when full
- **Why deque for frame buffers**: O(1) append/pop from both ends, automatic size management
- **Temporal data patterns**: Frame stacking implements sliding window over time series
- **np.concatenate vs np.stack**: Concatenate merges along existing axis, stack creates new axis
- **Shape management**: (84,84,1) Ã— 4 frames â†’ (84,84,4) via concatenation along last axis

### Dependency Management

- **Version conflict resolution**: opencv-python 4.12 requires NumPy 2.x, needed `"opencv-python<4.11"` for NumPy 1.x
- **Dependency cascades**: One constraint (NumPy<2.0 for gym) limits other package versions
- **Poetry constraint syntax**: Use quotes for complex version specs: `"package<version"`
- **Compatibility flags**: `apply_api_compatibility=True` bridges old/new Gym API differences

### API Compatibility Hell

- **Old vs New Gym API**: Old returns 4 values (obs, reward, done, info), new returns 5 (obs, reward, terminated, truncated, info)
- **env.reset() returns tuple**: New API returns `(obs, info)` tuple, not just `obs`
- **Wrapper composition affects API**: Each wrapper layer can modify return signatures
- **isinstance() for defensive coding**: Check if return is tuple before unpacking
- **API compatibility flag behavior**: `apply_api_compatibility=True` makes env output old 4-value API

## ðŸ’¡ Challenges & Solutions

### Challenge 1: Random Agent API Mismatches
**Problem**: Gym wrapper errors about "expected 5, got 4" vs "expected 4, got 5"
**Investigation**:
- TimeLimit wrapper expected 5 values but env returned 4
- Confusion about what `apply_api_compatibility=True` actually does
- Multiple attempts with different value counts failed

**Root Cause**: `apply_api_compatibility=True` makes env output OLD 4-value API (not new 5-value)
**Solution**: Use 4-value unpacking with compatibility flag, handle `done = terminated or truncated` manually

### Challenge 2: 32,000 Steps Per Episode!
**Problem**: Random agent Episode 4 reached 32,000 steps and still running
**Cause**: No maximum step limit - Mario wandering in circles indefinitely
**Learning**: Always set episode limits in RL to prevent runaway episodes
**Solution**: Added `max_steps_per_episode = 1000` and modified while condition: `while not done and step < max_steps_per_episode`

### Challenge 3: Jupyter Notebook Frame Collection Failures
**Problem**: Frame collection code ran without errors but `len(frames) == 0`
**Investigation**:
- Cells appeared to run but produced no output
- `frames` variable reported as "not defined" in subsequent cells
- Code looked correct but something blocked execution

**Root Causes**:
1. `env.reset()` returning tuple `(obs, info)` but code expected just `obs`
2. `frames = frames.append(obs)` bug - `append()` returns None!
3. Kernel state confusion from previous runs

**Solutions**:
- Unpack tuple: `observation, info = env.reset()`
- Fix append: `frames.append(obs)` without assignment
- Restart kernel and run all cells fresh

### Challenge 4: Wrapper Test Showed Wrong Shape
**Problem**: Test showed `(84, 4)` instead of expected `(84, 84, 4)`
**Cause**: Test code did `obs = env.reset()[0]`, taking first element of stacked array (first row!)
**Learning**: FrameStackWrapper.reset() already handles tuple unpacking internally
**Solution**: Remove `[0]` indexing - just use `obs = env.reset()`

### Challenge 5: Understanding Wrapper Design Pattern
**Problem**: Initially unclear how wrappers work and why we need multiple classes
**Learning Process**:
- Built GrayscaleWrapper first, understood basic pattern
- Saw how each wrapper modifies observation_space
- Realized composition power: `env â†’ Gray â†’ Resize â†’ Normalize â†’ Stack`

**Insight**: "Wrapper" means more than just decoration - it's a fundamental design pattern for composable functionality

### Evening Session (Post-Restart)
- [x] Troubleshoot wandb authentication issues âœ… 2025-12-31
- [x] Reauthenticate wandb with API key âœ… 2025-12-31
- [x] Add database schema migration for enhanced metrics âœ… 2025-12-31
- [x] Update random_agent.py to log all 10 info dict metrics âœ… 2025-12-31
- [x] Debug Python dictionary syntax errors (quotes, commas, typos) âœ… 2025-12-31
- [x] Successfully log enhanced baseline to wandb (13 metrics!) âœ… 2025-12-31
- [x] Define success criteria for Phase 3 (x_pos > 434, score â‰¥ 100, flag_get = True) âœ… 2025-12-31
- [x] Attempt video recording with gym.wrappers.RecordVideo (render mode issues) âœ… 2025-12-31
- [x] Debug render_mode compatibility with gym-super-mario-bros âœ… 2025-12-31
- [x] Implement manual frame collection with imageio âœ… 2025-12-31
- [x] Add tqdm progress bar for visual feedback âœ… 2025-12-31
- [x] Discover gym-super-mario-bros render limitations (static frames) âœ… 2025-12-31
- [x] **Decision: Proceed with metrics-only baseline, defer videos to Phase 3** âœ… 2025-12-31
- [x] **PHASE 2 COMPLETE!** âœ… 2025-12-31

## ðŸ”œ Tomorrow's Focus

- [ ] **Celebrate Phase 2 completion!** ðŸŽ‰
- [ ] Plan Phase 3: Simple RL Algorithm (DQN implementation)
- [ ] Research Stable-Baselines3 integration
- [ ] Design YAML configuration system for hyperparameters

## ðŸ”— Links & Context

- [[ProjectDocumentation]] - Updated Phase 2 tasks complete
- [[2025-12-30|Yesterday]] - Environment setup and first Phase 2 task
- Random agent script: `scripts/random_agent.py`
- Jupyter notebook: `notebooks/01_environment_exploration.ipynb`
- Preprocessing wrappers: `src/environments/wrappers.py`
- Python 3.11.14 venv working smoothly!

## ðŸ’» Code/Commands Used

```bash
# Jupyter Lab setup
poetry add --group dev jupyter
poetry add matplotlib  # For visualization
poetry run jupyter lab

# OpenCV installation with NumPy 1.x compatibility
poetry add "opencv-python<4.11"

# Running random agent
poetry run python scripts/random_agent.py

# Testing preprocessing wrappers
poetry run python src/environments/wrappers.py

# Git workflow (using lazygit for the first time!)
lazygit  # Interactive git UI
```

```python
# Random Agent - Final Working Version
import gym as gymnasium
import gym_super_mario_bros

env = gym_super_mario_bros.make("SuperMarioBros-v3", apply_api_compatibility=True)
observation = env.reset()

num_episode = 10
max_steps_per_episode = 1000

for episode in range(num_episode):
    observation = env.reset()
    done = False
    total_reward = 0
    step = 0

    while not done and step < max_steps_per_episode:
        action = env.action_space.sample()
        observation, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        step += 1
        done = terminated or truncated

    print(f"Episode {episode + 1}: Reward={total_reward}, Steps={step}")

env.close()
```

```python
# Preprocessing Pipeline - All Four Wrappers

# 1. GrayscaleWrapper: RGB â†’ Grayscale
class GrayscaleWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        self.observation_space = spaces.Box(
            low=0, high=255, shape=(240, 256, 1), dtype=np.uint8
        )

    def observation(self, obs):
        greyscale = np.dot(obs, [0.299, 0.587, 0.114])  # Weighted sum
        greyscale = np.expand_dims(greyscale, axis=-1)  # Add channel back
        return greyscale.astype(np.uint8)

# 2. ResizeWrapper: 240Ã—256 â†’ 84Ã—84
class ResizeWrapper(gym.ObservationWrapper):
    def __init__(self, env, size=84):
        super().__init__(env)
        self.size = size
        self.observation_space = spaces.Box(
            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8
        )

    def observation(self, obs):
        resized = cv2.resize(obs, (self.size, self.size))
        if len(resized.shape) == 2:  # cv2 might squeeze channel
            resized = np.expand_dims(resized, axis=-1)
        return resized.astype(np.uint8)

# 3. NormalizeWrapper: [0-255] â†’ [0-1]
class NormalizeWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(84, 84, 1), dtype=np.float32
        )

    def observation(self, obs):
        return obs / 255.0  # Simple division!

# 4. FrameStackWrapper: Stack 4 frames for temporal context
class FrameStackWrapper(gym.ObservationWrapper):
    def __init__(self, env, num_stack=4):
        super().__init__(env)
        self.num_stack = num_stack
        self.frames = deque(maxlen=num_stack)  # Sliding window
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(84, 84, num_stack), dtype=np.float32
        )

    def reset(self):
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
        # Fill buffer with first frame repeated
        for _ in range(self.num_stack):
            self.frames.append(obs)
        return self._get_stacked_frames()

    def observation(self, obs):
        self.frames.append(obs)  # Oldest auto-removed
        return self._get_stacked_frames()

    def _get_stacked_frames(self):
        return np.concatenate(list(self.frames), axis=-1)

# Usage: Chain them together!
env = gym_super_mario_bros.make("SuperMarioBros-v3", apply_api_compatibility=True)
env = GrayscaleWrapper(env)
env = ResizeWrapper(env)
env = NormalizeWrapper(env)
env = FrameStackWrapper(env, num_stack=4)

# Result: (240,256,3) uint8 RGB â†’ (84,84,4) float32 stacked grayscale!
```

## ðŸ“ Notes

**Epic productivity day!** Completed the random agent and built the entire preprocessing pipeline from scratch. This was a marathon session but incredibly rewarding.

**Learning style effectiveness**: The "guide, don't give code" approach worked well for the wrappers. Building each one step-by-step with hints and challenges helped me actually understand the design pattern, not just copy code. Made mistakes (lots of typos!), debugged them, and now I deeply understand how wrapper composition works.

**Key insight about wrappers**: The power isn't just in what each wrapper does - it's in how they COMPOSE. Each wrapper is simple (grayscale is literally 3 lines of logic), but chaining them creates a sophisticated preprocessing pipeline. This is the "Unix philosophy" applied to RL: small tools that do one thing well, composed together.

**Preprocessing impact**: Transformed raw observations from 184,320 values (240Ã—256Ã—3) to 28,224 values (84Ã—84Ã—4) - **6.5x reduction** while ADDING temporal context! This will dramatically speed up neural network training.

**Random agent baseline**: Established that random button-mashing gets ~380 average reward over 1000 steps. Any trained agent that beats this is learning something useful. The high variance (143-533) shows that random is indeed random - sometimes lucky, sometimes terrible.

**API compatibility saga**: Spent way too long debugging 4-value vs 5-value unpacking issues. Lesson learned: ALWAYS check what API version you're using, and whether compatibility flags convert TO or FROM the new API. Don't assume - test and verify!

**Jupyter debugging marathon**: The "frames not defined" issue taught me about Jupyter kernel state management. When things get weird, "Restart & Run All" is your friend. Also learned that cells can fail silently if you're not careful - always verify variables exist before using them in subsequent cells.

**Connection to cybersecurity goals**:
- Wrapper pattern applies to security tool composition (logging, monitoring, alerting layers)
- Preprocessing pipelines transfer to network packet feature extraction
- Baseline performance metrics analogous to security baseline measurements
- Systematic debugging approach critical for incident response

**Phase 2 COMPLETE!** Enhanced baseline with comprehensive metrics, wandb integration, and database schema ready for Phase 3. Built a random agent, explored the environment visually, and constructed a production-quality preprocessing pipeline. Ready for Phase 3: implementing actual RL algorithms!

**New Year's Eve reflection**: What a way to end 2025! Started this project 5 days ago, and already have:
- Complete ML infrastructure (database, W&B, pre-commit)
- Working environment with Python 3.11
- Random baseline agent
- Full preprocessing pipeline
- Jupyter exploration notebook

All while LEARNING deeply, not just copying tutorials. This hands-on approach is working - I understand the "why" behind every line of code.

### Evening Session Lessons (Post-Computer Restart)

**Wandb Authentication & API Keys:**
- wandb stores credentials in `~/.netrc` on Linux/Mac
- `wandb login --relogin` forces fresh authentication
- 401 errors mean "user is not logged in" - check API key
- wandb.init() creates local files even without cloud authentication

**Database Schema Evolution:**
- Used numbered migration files: `schema_migration_01.sql`
- ALTER TABLE adds columns to existing tables without data loss
- episodes table stores per-episode game metrics (distance, score, coins, etc.)
- training_metrics table stores per-step training metrics (loss, Q-values, etc.)
- Separation of concerns: game performance vs. ML training internals

**Enhanced Baseline Metrics (13 total):**
- Core: episode, reward, episode_length
- Position: x_pos (434 max), y_pos
- Performance: score (0-600 range), time remaining
- Collection: coins (always 0 for random)
- Status: life, status (small/tall/fireball), flag_get (always False)
- Level: world, stage (constants for 1-1)

**Video Recording Saga:**
- gym.wrappers.RecordVideo requires `render_mode='rgb_array'`
- gym-super-mario-bros doesn't support new gym render_mode properly
- RecordVideo creates .meta.json but fails to encode .mp4 files
- Manual imageio frame collection bypasses RecordVideo wrapper
- **Result: Videos created but contain static frames (render bug)**
- **Decision: Proceed without videos, rely on comprehensive metrics**

**Python Progress Bars (tqdm):**
- `from tqdm import tqdm`
- `for i in tqdm(range(n), desc="Description"):`
- Shows progress %, time elapsed, time remaining, iteration speed
- Nested bars possible for episode + step level tracking
- Improves UX for long-running training processes

**Success Criteria Established:**
For Phase 3 DQN to prove learning beyond random baseline:
- x_pos > 434 (beat random max distance)
- score â‰¥ 100 (efficient movement)
- flag_get = True (actually complete level)
- Consistency across episodes (not just luck)

**Video Recording Challenges:**
- gym-super-mario-bros is unmaintained (2019), doesn't support modern gym render_mode
- render_mode='rgb_array' returns static cached frames, not live gameplay
- RecordVideo wrapper fails to encode videos (double-close errors)
- Manual imageio collection works but frames don't update
- **Root cause**: Environment's internal rendering doesn't update with game state
- **Future options**: render_mode='human' + screen recording, or metrics-only approach

**Pragmatic Engineering Decision:**
Spent ~2 hours debugging video recording. Identified as environment limitation, not user error. Chose to:
- Accept comprehensive wandb metrics as primary proof of baseline
- Defer video recording to Phase 3 (screen recording as backup)
- Focus on actual ML work (training agents) vs. debugging legacy libraries
- Document the attempt and decision for future reference

**Connection to cybersecurity goals:**
- Database schema design applies to security event logging systems
- Metrics tracking mirrors SIEM/monitoring dashboards
- Pragmatic engineering decisions: know when to pivot vs. persevere
- Documentation of failed approaches prevents future wasted effort

_Session 1: 8:00 AM - 12:00 PM (Random agent + Jupyter notebook)_
_Session 2: 1:00 PM - 5:00 PM (Preprocessing pipeline)_
_Session 3: 6:00 PM - 10:00 PM (Enhanced baseline + wandb + video debugging)_
_Total: ~12 hours of focused learning and coding!_

**Happy New Year! ðŸŽ‰ Phase 2 complete - here's to training intelligent agents in Phase 3!**

[[daily]]
