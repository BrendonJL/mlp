# 2026-01-10 - Phase 4: PPO Implementation

## Session Focus
Implementing PPO (Proximal Policy Optimization) with parallel environments to compare against DQN baseline.

## Accomplishments

### 1. Created PPO Configuration (`configs/ppo_baseline.yaml`)
- Defined PPO-specific hyperparameters:
  - `n_steps: 1024` - steps per env before update
  - `n_epochs: 5` - passes over collected data
  - `batch_size: 128` - minibatch size
  - `clip_range: 0.2` - PPO clipping parameter
  - `ent_coef: 0.01` - entropy bonus for exploration
  - `gae_lambda: 0.95` - GAE parameter
  - `n_envs: 8` - parallel environments
- Kept same learning rate (0.0001) and gamma (0.99) for fair comparison

### 2. Updated Training Script (`src/training/train.py`)
- Added PPO import from Stable-Baselines3
- Implemented dynamic algorithm selection based on config:
  - Reads `algorithm` from config file ("PPO" or "DQN")
  - Loads appropriate hyperparameters section
  - Creates correct model type with algorithm-specific parameters
- Added error handling for unsupported algorithms (ValueError)
- Refactored to use `hyperparams` variable for cleaner code

### 3. Created Vectorized Environment (`src/environments/vec_mario_env.py`)
- New module for parallel environment creation
- Uses `SubprocVecEnv` for true multiprocessing parallelism
- Factory function pattern: `make_vec_mario_env(n_envs, game_version, action_space)`
- Each subprocess runs its own Mario environment independently

### 4. Integrated Vectorized Environments in Training
- Updated `train.py` to conditionally create:
  - `make_vec_mario_env()` for PPO (parallel)
  - `make_mario_env()` for DQN (single)
- Verified 8 parallel environments spawn correctly

### 5. Testing and Validation
- Short test run (10k timesteps): Verified pipeline works end-to-end
- Medium test run (50k timesteps): Checked CPU utilization
  - 8 envs at 82-95% CPU usage - good utilization without thrashing
  - Decided to keep 8 envs rather than scaling to 12
- Observed training metrics:
  - ~161 it/s during collection, drops to ~60 during updates
  - `approx_kl: 0.009` - healthy policy updates
  - `clip_fraction: 0.172` - appropriate clipping level
  - `explained_variance: 0.496` - value function learning

### 6. Full Training Run Started
- Kicked off 2M timestep PPO training run
- Estimated time: 4-5 hours (vs 12 hours for DQN!)
- Will enable direct comparison: PPO 2M vs DQN 2M

## Technical Learnings

### PPO vs DQN Key Differences
| Aspect | DQN | PPO |
|--------|-----|-----|
| Policy Type | Off-policy (replay buffer) | On-policy (fresh data) |
| Data Collection | Single env, reuses data | Parallel envs, discards after use |
| Network Output | Q-values | Policy + Value (actor-critic) |
| Key Innovation | Experience replay, target network | Clipped surrogate objective |

### Vectorized Environment Pattern
```python
def make_vec_mario_env(n_envs, game_version, action_space):
    def make_env(_):  # _ = unused rank parameter
        def _init():
            return make_mario_env(game_version, action_space)
        return _init
    return SubprocVecEnv([make_env(i) for i in range(n_envs)])
```
- Factory function returns callable for each subprocess
- `SubprocVecEnv` spawns separate processes for true parallelism

### PPO Training Metrics to Watch
- **approx_kl**: Should stay < 0.02 (policy stability)
- **clip_fraction**: 10-20% is healthy (clipping working but not excessive)
- **explained_variance**: Approaches 1.0 as value network improves
- **entropy_loss**: Higher = more exploration

### CPU Utilization Sweet Spot
- 8 envs at 82-95% CPU is optimal
- Maxing at 100% causes context-switching overhead
- Leave headroom for gradient updates and OS tasks

## Files Created/Modified

### New Files
- `configs/ppo_baseline.yaml` - PPO experiment configuration
- `src/environments/vec_mario_env.py` - Vectorized environment wrapper

### Modified Files
- `src/training/train.py` - Multi-algorithm support (PPO + DQN)

## Next Steps
1. Wait for 2M PPO training to complete (~4-5 hours)
2. Analyze results and compare to DQN baseline
3. Create comparison notebook/visualizations
4. Plan Phase 5: Reward shaping and hyperparameter tuning

## Session Statistics
- **Time spent**: ~3 hours (implementation + testing)
- **Training time**: ~4-5 hours (running in background)
- **New code**: ~50 lines (vec_mario_env.py, train.py modifications)
- **Experiments**: 2 test runs (10k, 50k) + 1 full run (2M in progress)

---

**Training Status**: 2M PPO run in progress
**Next Session**: Analyze PPO results, compare to DQN, plan reward shaping
