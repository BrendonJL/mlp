# 2026-01-11 - Phase 4 Complete: PPO Training Analysis

## Session Focus
Analyzing PPO training results and diagnosing policy collapse.

## Key Discovery: Policy Collapse

### The Problem
After completing the 2M timestep PPO training run (~10.2 hours), evaluation revealed the agent had learned **degenerate behavior**:
- Jumps up, hits the first block
- Turns around and runs LEFT (backwards!)
- Sits in the corner until killed by Goomba

### Root Cause Analysis

**Checkpoint comparison revealed policy collapse:**

| Checkpoint | Final X | Max X | Behavior |
|------------|---------|-------|----------|
| 800k steps | 353 | 353 | Moving right (correct!) |
| 1.6M steps | 66 | 313 | Goes right then retreats |
| 2M steps (final) | ~40 | ~40 | Immediately runs backwards |

The agent was learning correctly at 800k steps, but training continued too long and the policy collapsed.

### Warning Signs We Missed
- **Value loss spikes**: During training, value_loss jumped to 30-50 periodically
- **No episode metrics logged**: Callbacks weren't working with vectorized environments
- **Only system metrics in W&B**: CPU/memory were logged, but not rewards/distances

## Callback Bug Identified

Both `WandbCallback` and `DatabaseCallback` only check environment index `[0]`:

```python
# Current (broken for VecEnv):
if self.locals["dones"][0]:
    episode_info = self.locals["infos"][0].get("episode")
```

With 8 parallel environments, this misses 7/8 of episode completions! The fix requires iterating over ALL environments.

## Technical Learnings

### PPO Policy Collapse Causes
1. **Learning rate too high** (0.0001) - PPO is sensitive, 0.00003 often works better
2. **Entropy coefficient too low** (0.01) - policy became overconfident too quickly
3. **No early stopping** - training continued past the point of good performance
4. **No monitoring** - couldn't see the collapse happening in real-time

### DQN vs PPO Evaluation Difference
- **DQN**: `deterministic=False` often performs better (allows learned exploration)
- **PPO**: `deterministic=True` typically performs better (takes most confident action)

This is because DQN outputs Q-values (needs sampling for diversity) while PPO directly outputs action probabilities (sampling adds noise during eval).

### Vectorized Environment Gotchas
- `SubprocVecEnv` returns arrays: `dones[n_envs]`, `infos[n_envs]`
- Callbacks must iterate over all environments to catch all episode completions
- Standard SB3 callbacks may not work correctly without modification

## Phase 4 Outcome

**Status: Complete (with documented failure)**

Despite the policy collapse, Phase 4 achieved its learning objectives:
- [x] Implemented PPO with parallel environments
- [x] Built vectorized environment wrapper (SubprocVecEnv)
- [x] Updated training pipeline for multi-algorithm support
- [x] Learned about on-policy vs off-policy data collection
- [x] Discovered policy collapse failure mode (valuable lesson!)
- [x] Identified callback bugs for Phase 5 fix

**Artifacts Created:**
- `configs/ppo_baseline.yaml` - PPO configuration
- `src/environments/vec_mario_env.py` - Vectorized environment wrapper
- `models/ppo_baseline_world1-1_800000_steps.zip` - Best PPO checkpoint (before collapse)
- `models/ppo_baseline_world1-1_final.zip` - Collapsed policy (for reference)

## Phase 5 Priorities

1. **Fix callbacks** - Iterate over all vectorized environments
2. **Hyperparameter tuning**:
   - Lower learning rate: 0.0001 → 0.00003
   - Higher entropy: 0.01 → 0.02
   - Consider early stopping based on evaluation
3. **Reward shaping** - Custom rewards to encourage forward progress
4. **Proper monitoring** - Ensure W&B logs episode metrics for early collapse detection

## Session Statistics
- **Analysis time**: ~2 hours
- **Key discovery**: Policy collapse between 800k-2M steps
- **Bugs found**: 2 (WandbCallback, DatabaseCallback)
- **Learning value**: High - failure modes are important to understand!

---

**Phase 4 Status**: COMPLETE
**Next Phase**: 5 - Reward Shaping & Hyperparameter Tuning
