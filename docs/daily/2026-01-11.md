# 2026-01-11 - Phase 5: Infrastructure Fixes, Reward Shaping & Full Training Run

## Session Focus
Fixed critical infrastructure bugs, implemented comprehensive reward shaping, and launched full PPO training run.

## Major Accomplishments

### Part A: Infrastructure Fixes

#### 1. Fixed Vectorized Environment Callbacks
**Problem:** `WandbCallback` and `DatabaseCallback` only checked `dones[0]`, missing 7/8 of episode completions with parallel environments.

**Solution:** Updated both callbacks to iterate over ALL environments:
```python
for i in range(len(dones)):
    if dones[i]:
        episode_info = infos[i].get("episode")
        # ... log episode
```

#### 2. Added VecMonitor for Episode Tracking
**Problem:** Individual `Monitor` wrapper incompatible with our old-gym-based wrappers.

**Solution:** Used `VecMonitor` to wrap the entire `SubprocVecEnv`:
```python
vec_env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
vec_env = VecMonitor(vec_env)  # Tracks episode stats for all envs
```

#### 3. Fixed Gym/Gymnasium API Compatibility
**Problem:** `CompatibilityWrapper` only handled `reset()`, not `step()`. Old gym returns 4 values, new gymnasium expects 5.

**Solution:** Added `step()` method to `CompatibilityWrapper`:
```python
def step(self, action):
    result = self.env.step(action)
    if len(result) == 4:  # Old gym API
        obs, reward, done, info = result
        return obs, reward, done, False, info  # Convert to 5-tuple
    return result
```

#### 4. Reduced Parallel Environments
Changed from 8 to 4 environments to prevent CPU throttling (was causing 10+ hour training times).

### Part B: Hyperparameter Tuning (in ppo_v2.yaml)
- `learning_rate`: 0.0001 → 0.00003 (more stable for PPO)
- `ent_coef`: 0.01 → 0.02 (more exploration)
- `n_envs`: 8 → 4 (prevent CPU throttling)

### Part C: Reward Shaping

Created comprehensive `RewardShapingWrapper` with:

| Feature | Value | Purpose |
|---------|-------|---------|
| Forward bonus | +0.1/pixel | Reward rightward movement |
| Backward penalty | -0.1/pixel | Discourage retreating |
| Idle penalty | -0.2/step | Penalize standing still |
| Death penalty | -50 | Discourage dying |
| Max stuck steps | 150 | Early termination when stuck |
| Milestone bonuses | 650→+150, 900→+100, 1200→+150, 1600→+200, 2000→+250 | Reward reaching checkpoints |

**Key Discovery:** Agent kept getting stuck at x=594. Added early termination (150 stuck steps) and milestone bonus at x=650 to incentivize breaking past this obstacle.

## Technical Learnings

### SubprocVecEnv and Print Statements
Print statements in subprocess environments don't appear in the main terminal! Had to add debugging to callbacks (main process) instead of wrappers (subprocesses).

### Reward Shaping vs Episode Termination
Reward shaping changes the reward signal but doesn't make episodes end faster. For faster learning, needed BOTH:
1. Shaped rewards (penalize bad behavior)
2. Early termination (more episodes = more learning opportunities)

### Episode Frequency Impact
- Before fixes: ~1 episode per 32k steps (timeout only)
- After early termination: ~3 episodes per 9k steps
- ~10x more learning feedback!

### Why PPO Needed More Help Than DQN
1. **Replay buffer**: DQN remembers rare successes, PPO forgets immediately
2. **Exploration**: DQN's epsilon-greedy randomly stumbles past obstacles
3. **Sample efficiency**: PPO discards data after each update, needs clearer signals

## Files Created/Modified

### New Files
- `configs/ppo_v2.yaml` - Tuned PPO configuration

### Modified Files
- `src/training/callbacks.py` - Fixed vectorized environment iteration
- `src/environments/wrappers.py` - Added `CompatibilityWrapper.step()`, created `RewardShapingWrapper`
- `src/environments/vec_mario_env.py` - Added `VecMonitor` wrapping
- `src/environments/mario_env.py` - Added `RewardShapingWrapper` to pipeline

## Training Run Started
- **Config:** `configs/ppo_v2.yaml`
- **Timesteps:** 2,000,000
- **Early results:** 8 episodes completed by 12k steps, agent passed x=594!
- **Expected completion:** Overnight

## Tomorrow's Tasks
1. Evaluate trained PPO v2 model
2. Clean up PostgreSQL database (remove test runs)
3. Create comparison notebook: PPO v2 vs DQN baseline
4. Generate visualizations and graphs
5. Update README with results

## Session Statistics
- **Time spent:** ~4 hours
- **Bugs fixed:** 5+ (callbacks, API compatibility, reward shaping)
- **New code:** ~100 lines (RewardShapingWrapper, callback fixes)
- **Key insight:** On-policy algorithms like PPO need stronger reward signals than off-policy algorithms like DQN

---

**Training Status:** 2M PPO v2 run in progress
**Next Session:** Evaluation, comparison notebook, visualizations
