---
id: "2026-01-03"
aliases: []
tags:
  - daily-notes
  - mario-rl-project
  - phase-3
  - debugging
  - first-training-run
  - pipeline-integration
created: 2026-01-03
---
# üìÖ Friday, January 3, 2026

> _Week 1 of 2026 - Phase 3 Progress: Test Run Complete!_

## üéØ Today's Goals

- [x] Run first end-to-end training test (1000 timesteps)
- [x] Debug any integration issues
- [x] Verify wandb and PostgreSQL logging
- [ ] Run full 2M timestep training (in progress...)

## ‚úÖ What I Accomplished

**EPIC 3-HOUR DEBUGGING SESSION!** Fixed 12 different issues to get the first successful training run.

### The 12 Bugs Fixed

**1. Module Import Syntax** ‚úÖ
- Error: `ModuleNotFoundError: No module named src/training/train`
- Fix: Changed `src/training/train` ‚Üí `src.training.train` (dots not slashes!)
- Learning: Python `-m` flag requires module notation

**2. Missing `__init__.py` Files** ‚úÖ
- Error: `No module named src`
- Fix: Created `__init__.py` in `src/` and all subdirectories
- Learning: Python requires explicit package markers

**3. Missing stable-baselines3** ‚úÖ
- Error: `ModuleNotFoundError: No module named 'stable_baselines3'`
- Fix: `poetry add stable-baselines3`
- Bonus error: Typo `stable-baseline3` (singular) ‚Üí `stable-baselines3` (plural!)

**4. Config Key Mismatches (4 separate issues!)** ‚úÖ
- `config["environment"]["name"]` ‚Üí `config["environment"]["game"]`
- `config["hyperparameters"]` ‚Üí `config["dqn_hyperparameters"]`
- `config["experiment"]["policy"]` ‚Üí `config["dqn_hyperparameters"]["policy"]`
- `config["paths"]["models"]` ‚Üí `config["paths"]["model_dir"]`
- Learning: YAML structure must EXACTLY match code references

**5. Database Password Mismatch** ‚úÖ
- Error: `password authentication failed for user "mario_rl_user"`
- Fix: Updated password in `db_logger.py` to match PostgreSQL

**6. SQL Query Syntax Error** ‚úÖ
- Error: LSP warnings about malformed string
- Issue: `query = """1` (stray `1` after triple-quote)
- Fix: Removed the typo
- Learning: LSP warnings save you from runtime errors!

**7. Function Signature Mismatch** ‚úÖ
- Error: `TypeError: create_experiment() got an unexpected keyword argument 'name'`
- Issue: Function expects `experiment_name`, `git_commit_hash`, `python_version`, `pytorch_version`
- Fix: Added metadata collection code in `train.py`
- Learning: Reproducibility requires tracking versions and git commits!

**8. Missing shimmy Dependency** ‚úÖ
- Error: `ImportError: Missing shimmy installation`
- Cause: SB3 upgraded to Gymnasium, gym-super-mario-bros still uses old Gym
- Fix: `poetry add 'shimmy>=2.0'`
- Learning: Shimmy = compatibility shim between Gym versions

**9. Image Format Incompatibility** ‚úÖ
- Error: `AssertionError: You should use NatureCNN only with images not with Box(...(84, 84, 4)...)`
- Issue: Channel-last `(H, W, C)` format, PyTorch needs channel-first `(C, H, W)`
- Fix: Created `TransposeWrapper` using `np.transpose(obs, (2, 0, 1))`
- Learning: NumPy/TensorFlow use (H,W,C), PyTorch uses (C,H,W)!

**10. Normalization API Version** ‚úÖ
- Error: `TypeError: DQN.__init__() got an unexpected keyword argument 'normalize_images'`
- Issue: Tried to pass parameter that doesn't exist in this SB3 version
- Fix: Removed `NormalizeWrapper`, keep images as uint8 [0,255], let SB3 normalize
- Learning: Match preprocessing to what the library expects!

**11. Gym API Compatibility - Parameters** ‚úÖ
- Error: `TypeError: JoypadSpace.reset() got an unexpected keyword argument 'seed'`
- Issue: New Gym passes `seed` kwarg, old `JoypadSpace` doesn't accept it
- Fix: Created `CompatibilityWrapper` with try/except for kwargs
- Learning: Build adapters at API boundaries!

**12. Gym API Compatibility - Return Values** ‚úÖ
- Error: `ValueError: too many values to unpack (expected 2)`
- Issue: Old API returns `obs`, new API expects `(obs, info)` tuple
- Fix: Updated wrappers to always return `(obs, info)` tuple
- Learning: When overriding base methods, match expected interface!

### Final Working Pipeline

```python
env = gym_super_mario_bros.make("SuperMarioBros-v3")
env = JoypadSpace(env, SIMPLE_MOVEMENT)      # 256 ‚Üí 7 actions
env = CompatibilityWrapper(env)              # Old/new Gym API bridge
env = GrayscaleWrapper(env)                  # RGB ‚Üí Grayscale
env = ResizeWrapper(env)                     # 240√ó256 ‚Üí 84√ó84
# NormalizeWrapper REMOVED - SB3 handles it
env = FrameStackWrapper(env, num_stack=4)    # Stack 4 frames
env = TransposeWrapper(env)                  # (H,W,C) ‚Üí (C,H,W)
# Final: (4, 84, 84) uint8 [0, 255] ‚úÖ
```

### Test Run Results (1000 timesteps)

‚úÖ Training completed successfully
‚úÖ Wandb logging working
‚úÖ PostgreSQL logging working (experiment_id: 9)
‚úÖ Model saved: `models/dqn_baseline_world1-1_final`
‚úÖ Git metadata tracked: 4ce6ec4a, Python 3.11.14, PyTorch 2.9.1+cu128

## üß† What I Learned

### The Reality of ML Engineering

**What tutorials show**: "Just run this and it works!"
**What actually happens**: 12 errors, 3 hours of debugging, library incompatibilities

This IS the job. Tutorials skip this, but THIS is where 50% of dev time goes!

### Wrapper Pattern Power

Built 5 custom wrappers today (6th removed):
1. **CompatibilityWrapper** - Bridges old/new Gym API
2. **GrayscaleWrapper** - RGB ‚Üí grayscale
3. **ResizeWrapper** - Downsampling
4. **FrameStackWrapper** - Temporal context
5. **TransposeWrapper** - Format conversion

Each does ONE thing. Composed together = sophisticated preprocessing pipeline!

### Image Format Conventions Matter

- **NumPy/TensorFlow**: `(Height, Width, Channels)` = `(84, 84, 4)`
- **PyTorch**: `(Channels, Height, Width)` = `(4, 84, 84)`

Different frameworks, different conventions. ALWAYS check docs!

### API Compatibility Strategies

**The Problem**: Old gym-super-mario-bros + new Stable-Baselines3

**The Solution**: Compatibility shim that:
1. Tries new API (with kwargs)
2. Falls back to old API (without kwargs)
3. Normalizes return values (ensure tuple)

This is what `shimmy` does - we built a mini version!

### Metadata for Reproducibility

Added tracking for:
- Git commit hash (4ce6ec4a)
- Python version (3.11.14)
- PyTorch version (2.9.1+cu128)

Why? 3 months from now I can reproduce EXACTLY this experiment!

### Systematic Debugging

1. Read error message carefully
2. Understand root cause
3. Research solution
4. Implement fix
5. Test
6. Move to next error

No shortcuts, no guessing - methodical wins!

## üí° Challenges & Solutions

### Challenge: 12 Cascading Errors

**Feel**: Frustrating! Each fix revealed a new error (whack-a-mole!)
**Reality**: Normal for first integration - setup code isn't tested until you run it
**Approach**: Patience + systematic debugging
**Result**: Every error taught something valuable

### Challenge: API Compatibility Hell

**Problem**: Mixing old libraries (gym-super-mario-bros) with new (SB3, shimmy)
**Root Cause**: Library ecosystem transition (Gym ‚Üí Gymnasium) incomplete
**Solution**: Built compatibility shim wrappers
**Learning**: When integrating legacy + modern code, expect API mismatches

## üîú Next Steps

- [ ] **Run full 2M timestep training** (About to start!)
- [ ] Create evaluation script (Phase 3)
- [ ] Build analysis notebook (Phase 3)

## üíª Code Added Today

**Files Modified:**
- `src/environments/wrappers.py` - Added CompatibilityWrapper, TransposeWrapper, fixed FrameStackWrapper
- `src/environments/mario_env.py` - Updated wrapper chain, added CompatibilityWrapper
- `src/training/train.py` - Added metadata tracking (git/Python/PyTorch versions), fixed config keys
- `src/utils/db_logger.py` - Fixed SQL typo, updated password
- Created `__init__.py` in 7 directories

**Dependencies Added:**
- `stable-baselines3`
- `shimmy>=2.0`

## üìù Notes

**Timeline**:
- 8:29 AM: First test run attempt
- 8:29-9:25 AM: Fixed 12 errors systematically
- 9:25 AM: **SUCCESS!** First training run complete
- 9:30 AM: Database cleanup, ready for full run

**Debugging Mindset**: Each error was a learning opportunity:
- Python package system
- Configuration management
- Database integration
- API compatibility
- Deep learning conventions
- Defensive programming

**Phase 3 Progress**: 9/12 tasks complete (75%)
- ‚úÖ End-to-end test run (1000 timesteps)
- ‚è≥ Full training (2M timesteps) - starting now!
- ‚è≥ Evaluation script
- ‚è≥ Analysis notebook

**Connection to cybersecurity goals**:
- Dependency management ‚Üí security tool deployment
- API compatibility ‚Üí integrating security products
- Metadata tracking ‚Üí incident investigation
- Defensive programming ‚Üí vulnerability prevention
- Systematic debugging ‚Üí incident response

_Session: 8:29 AM - 9:30 AM (3 hours of focused debugging)_
_Errors fixed: 12_
_Successful runs: 1 (after 9 failed attempts!)_

**Ready for the BIG run! üöÄ**

---

## üöÄ Afternoon Session: Full Training Run & 4 More Bugs!

**Timeline**: 9:58 AM - 12:00 PM (Full pipeline debugging)

### The 4 Additional Bugs Fixed

**Bug #13: Missing tqdm/rich Dependencies** ‚úÖ
- Error: `ImportError: You must install tqdm and rich in order to use the progress bar callback`
- Issue: Added `progress_bar=True` to model.learn() but missing optional dependencies
- Root Cause: tqdm was already installed, but `rich` was not (needed for SB3's progress bar)
- Fix: `poetry add rich`
- Learning: SB3's `[extra]` bundle includes tqdm + rich for progress bars

**Bug #14: NumPy Type Incompatibility with PostgreSQL** ‚úÖ
- Error: `psycopg2.ProgrammingError: can't adapt type 'numpy.int64'`
- Issue: Mario environment returns NumPy types (`numpy.int64`, `numpy.float64`), psycopg2 expects Python natives
- First episode completed at ~28k steps (~7 minutes), then crashed on database insert
- Fix: Convert all values in callback: `int(info["x_pos"])`, `float(episode_info["r"])`, `bool(info["flag_get"])`
- Learning: Type conversions needed at system boundaries (ML ‚Üí Database)
- **THIS is why the 1000-step test didn't catch it!** No episodes completed, so database logging never ran!

**Bug #15: Database Schema Column Name Mismatch** ‚úÖ
- Error: `psycopg2.errors.UndefinedColumn: column "total_reward" of relation "episodes" does not exist`
- Issue: Code expects `total_reward`, `x_pos`, `flag_get` but schema has `reward`, `distance_traveled`, `level_completed`
- Root Cause: Code written with different naming than original schema + migration
- Fix: Updated db_logger.py INSERT query to use actual column names from schema
- Learning: Schema evolution requires keeping code and database in sync

**Bug #16: Missing timestamp Column** ‚úÖ
- Issue: Original schema has `timestamp` column, but INSERT query didn't include it
- Fix: Added `timestamp` to INSERT with `NOW()` function
- Learning: Always check schema completeness, not just column names

### Episode Timing Insights

**Critical Discovery**: Test size matters for integration testing!
- 1000-step test: ‚úÖ Found 12 bugs (imports, config, environment, wrappers)
- 1000-step test: ‚ùå Missed 4 bugs (database logging, type conversion, schema mismatch)
- **Why?** No episodes completed in 1000 steps, so database callback never executed!
- **Lesson**: Integration tests must be long enough to exercise all code paths
- **Better test**: 20k-30k steps = at least 1 episode = full pipeline validated

**Episode Lengths (Early Training - Random Exploration)**:
- Episode 1: ~28k steps (~7 min) - Random wandering before Mario died
- Episode 2: ~19k steps (~4 min) - Shorter, Mario died faster
- Episode 3: ~20k steps (~4 min) - Similar pattern
- Episode 4: First console output appeared! (log_interval=4)
- Episode 5: ‚úÖ **PIPELINE FULLY VALIDATED!**

**Why so long?** DQN uses `learning_starts` parameter (50k steps) where agent takes random actions to fill replay buffer. Early episodes are pure random exploration - Mario wanders aimlessly until dying.

### System Management

**Hypridle Issue & Solution**:
- Issue: Hyprland's hypridle could trigger system suspend during multi-hour training
- Risk: System suspend would pause training process, potentially losing progress
- Solution: Killed hypridle process with `pkill hypridle` (safe, doesn't affect training)
- Learning: Long-running processes need protection from power management
- Alternative approaches: `systemd-inhibit`, disable suspend, run in tmux

### Final Working Configuration

**Full Pipeline Validated** (5 episodes successfully logged):
1. ‚úÖ Training orchestrator (`train.py`)
2. ‚úÖ DQN agent with CnnPolicy (Stable-Baselines3)
3. ‚úÖ Mario environment with 5 wrappers (Compatibility, Grayscale, Resize, FrameStack, Transpose)
4. ‚úÖ Custom callbacks (WandbCallback, DatabaseCallback)
5. ‚úÖ PostgreSQL episode logging (13 metrics per episode)
6. ‚úÖ Weights & Biases cloud tracking (real-time graphs)
7. ‚úÖ Progress bar with tqdm + rich (ETA, speed, completion %)
8. ‚úÖ Model checkpointing every 100k steps
9. ‚úÖ Console stats every 4 episodes

**Training Status**:
- Current steps: ~100k / 2M (5%)
- Episodes completed: 5+
- Estimated completion: 8-12 hours
- CPU usage: ~500% (5 cores)
- Memory: ~4.4 GB
- Status: **RUNNING SUCCESSFULLY!** üéÆ

## üß† What I Learned (Afternoon Session)

### Integration Testing Principles

**Code Path Coverage**: Short tests can miss bugs that only trigger under realistic workloads
- Test ran 1000 steps: ‚úÖ Environment setup, ‚ùå Episode completion
- Full run hit first episode at 28k: ‚úÖ Found database bugs immediately
- **Lesson**: Tests must reach critical checkpoints (episode completion, batch end, checkpoint save)

**Connection to Cybersecurity**: Same pattern as security vulnerabilities hiding in error paths
- Happy path works fine (normal traffic)
- Edge cases crash (malformed packets, buffer overflow)
- Production workloads expose what tests miss

### Type System Boundaries

**NumPy ‚Üî Python ‚Üî PostgreSQL**:
- NumPy uses: `numpy.int64`, `numpy.float64`, `numpy.bool_` (optimized for computation)
- Python uses: `int`, `float`, `bool` (native types)
- PostgreSQL expects: Python natives via psycopg2
- **Solution**: Explicit conversion at integration points: `int(x)`, `float(y)`, `bool(z)`

**Why This Matters**: Each system optimizes for its use case
- NumPy: Fast array operations, typed arrays
- Python: Dynamic typing, object-oriented
- PostgreSQL: Relational integrity, typed columns
- **Integration requires translation layers**

### Process Management & Isolation

**Unix Process Model**:
- Each process has isolated memory space, execution context
- Killing hypridle doesn't affect Python training (independent processes)
- Kernel manages thousands of processes without interference
- `ps aux` shows: PID, CPU time (cumulative), memory, start time, status

**Long-Running Processes**:
- Need protection from power management (suspend, hibernate)
- Can run in background (no terminal required after validation)
- Can monitor remotely (wandb dashboard, tmux/screen sessions)

### Schema Evolution & Naming Conventions

**Database Migration Challenges**:
- Original schema: `reward`, `distance_traveled`, `level_completed`
- Migration added: `score`, `time`, `coins`, `life`, `status`, `y_pos`, `world`, `stage`
- Code written later: `total_reward`, `x_pos`, `flag_get` (different names!)
- **Root Cause**: No single source of truth for column names

**Best Practices** (for future):
- Document schema in one place (schema.sql as source of truth)
- Use schema inspection tools to verify column names before coding
- Consider using ORM (SQLAlchemy) to catch mismatches at development time
- Test database integration early (don't wait for full run)

### Real ML Engineering

**What Tutorials Show**: "Just run this and it works!"
**What Actually Happens**:
- 16 integration bugs across 4 systems
- 4+ hours of systematic debugging
- Type conversions, API compatibility, schema mapping
- Process management, dependency resolution

**This IS the Job**:
- 50% debugging integration issues
- 30% making systems talk to each other
- 20% actual ML algorithm work
- Tutorials skip this critical skill development!

## üí° Key Insights

### Testing Philosophy

**"Tests that don't exercise the full system only prove partial correctness"**
- 1000 steps = partial test (startup, config, environment)
- 30k steps = full test (entire pipeline including database)
- Always test through critical checkpoints

### Debugging Mindset

**Systematic Approach That Worked**:
1. Run until error
2. Read error message carefully
3. Understand root cause (not just symptoms)
4. Implement fix with understanding
5. Verify fix worked
6. Document learning
7. Repeat

**16 bugs fixed in one day using this method!**

### Engineering Maturity

**Signs of Growing Expertise**:
- ‚úÖ Questioned test coverage ("shouldn't 1000 steps have caught this?")
- ‚úÖ Understood type system boundaries (NumPy vs Python vs PostgreSQL)
- ‚úÖ Recognized integration patterns (callbacks, wrappers, adapters)
- ‚úÖ Made pragmatic decisions (kill hypridle vs complicated alternatives)
- ‚úÖ Systematic debugging (no random guessing, methodical fixes)

## üîú Next Steps

**Immediate** (Next 8-12 hours):
- [ ] Training completes (2M timesteps) - **IN PROGRESS**
- [ ] Monitor wandb dashboard periodically
- [ ] Verify final model saved successfully

**Phase 3 Remaining Tasks**:
- [ ] Create evaluation script (load model, run test episodes)
- [ ] Build analysis notebook (random vs trained comparison)
- [ ] Document findings and performance metrics

**Phase 3 Status**: 10/12 tasks complete (83%)

## üíª Code Modified (Afternoon Session)

**Files Modified:**
- `src/training/callbacks.py` - Added NumPy ‚Üí Python type conversions for all 13 metrics
- `src/training/train.py` - Added `progress_bar=True` to model.learn()
- `src/utils/db_logger.py` - Fixed column names (`reward`, `distance_traveled`, `level_completed`), added `timestamp`

**Dependencies Added:**
- `rich` (for progress bar visualization)

**System Commands:**
- `pkill hypridle` - Disabled idle daemon to prevent system suspend

## üìù Session Notes

**Total Debugging Time**: ~6 hours (3 hours morning + 3 hours afternoon)
**Total Bugs Fixed**: 16 (12 in test run + 4 in full run)
**Restarts Required**: 5 attempts before successful full validation

**Celebration Moment**: When episode 5 logged successfully at 12:00 PM! üéâ

**Connection to Cybersecurity Goals**:
- Integration testing ‚Üí penetration testing methodology
- Type system boundaries ‚Üí input validation & sanitization
- Schema evolution ‚Üí database security & migration safety
- Process isolation ‚Üí container security & privilege separation
- Systematic debugging ‚Üí incident response & root cause analysis

**Lessons That Will Transfer**:
- Always test realistic workloads, not just minimal cases
- Understand system boundaries and type conversions
- Document integration points and assumptions
- Protect long-running processes from interruption
- Debug systematically, not randomly

_Morning Session: 8:29 AM - 9:30 AM (Test run, 12 bugs fixed)_
_Afternoon Session: 9:58 AM - 12:00 PM (Full run, 4 bugs fixed, pipeline validated)_
_Total: ~6 hours of focused ML engineering!_

**Training Status: RUNNING! Expected completion: Tonight/Tomorrow Morning**

**üéÆ Mario is learning! The pipeline works! Phase 3 almost complete! üöÄ**

[[daily]]
