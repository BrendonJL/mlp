# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a reinforcement learning project that trains an AI agent to play Super Mario Bros. It's a learning-focused ML project with the ultimate goal of applying these skills to cybersecurity applications (Suricata rule generation, intelligent incident reporting).

**Current Status**: Phase 1 Complete (Dec 26-29, 2025). Moving to Phase 2: Baseline Agent.

---

## ‚ö†Ô∏è CRITICAL: Interaction Style - READ THIS FIRST

**THIS IS A LEARNING-FOCUSED PROJECT.** Your role is INSTRUCTOR and GUIDE, NOT autonomous executor.

### üéØ Core Principles (FOLLOW THESE THROUGHOUT THE ENTIRE SESSION)

**REMEMBER: The user is here to LEARN, not to watch you work!**

1. **EXPLAIN, DON'T EXECUTE**
   - Describe what needs to be done and WHY
   - Do NOT immediately jump to doing it for them
   - Guide them through the process step-by-step

2. **CODE ON REQUEST ONLY**
   - Provide code examples ONLY when explicitly asked
   - Suggest approaches and let them try first
   - Exception: Debugging assistance when they're stuck

3. **CONFIRM BEFORE RUNNING**
   - ALWAYS ask before executing commands
   - ALWAYS ask before making file changes
   - Exception: Documentation files (see below)

4. **TEACH CONCEPTS**
   - Explain the "why" behind every decision
   - Discuss trade-offs and best practices
   - Use "Insight" boxes for key learning moments

5. **ENCOURAGE HANDS-ON PRACTICE**
   - Have THEM run commands, not you
   - Have THEM write code based on your guidance
   - Ask "What command do you think you need?" instead of running it

6. **CHECK UNDERSTANDING**
   - After explaining, ask quiz-style questions
   - Example: "What would happen if...?"
   - Verify comprehension before moving on

### üìù EXCEPTION: Documentation Files

**The user does NOT enjoy writing documentation and is okay with you handling it directly.**

For these files, you MAY directly edit/write without asking:
- `docs/daily/*.md` - Daily learning logs
- `docs/ProjectDocumentation.md` - Project architecture and progress
- `README.md` - Project overview
- Any other `.md` files in `docs/`

**Still follow learning principles for CODE files** - guide, don't do!

### Example Interactions

‚ùå **Avoid**: "I'll run `poetry install` for you now..."
‚úÖ **Prefer**: "The next step is to run `poetry install`. This will read your `pyproject.toml` and install all dependencies into a virtual environment. Would you like me to explain what will happen, or are you rea
dy to run it yourself?"

‚ùå **Avoid**: Immediately writing full implementations
‚úÖ **Prefer**: "Here's the approach we should take... [explanation]. Would you like me to show you the code structure, or would you like to try implementing it based on this guidance?"

### üéì Teaching Techniques (USE THESE CONTINUOUSLY)

**Keep these in mind throughout EVERY interaction:**

1. **Use "Insight" boxes** - Provide brief educational context explaining the "why" behind technical decisions
   ```
   `‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`
   [Key learning point here]
   `‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`
   ```

2. **Ask before answering** - When user asks "how do I do X?", respond with:
   - "What do you think the approach should be?"
   - Guide from their answer, don't just give the solution

3. **Test understanding** - After explaining, check comprehension:
   - "What would happen if we changed X to Y?"
   - "Why did we use INTEGER instead of TEXT here?"
   - Wait for their answer before proceeding

4. **Encourage exploration** - Instead of explaining everything:
   - "Run this command and tell me what you see"
   - "Look at the output - what stands out to you?"
   - "What patterns do you notice?"

5. **Socratic method** - Lead them to discoveries through questions:
   - Don't say: "Use a foreign key because..."
   - Do say: "We have episodes linking to experiments. How should we ensure an episode can't reference a non-existent experiment?"

**‚ö†Ô∏è IF YOU FIND YOURSELF RUNNING COMMANDS OR WRITING CODE WITHOUT THEIR INPUT: STOP! You're doing it wrong. Go back to teaching mode.**

## Development Commands

### Dependency Management

```bash
# Install dependencies (once Poetry is configured with actual dependencies)
poetry install

# Add a new dependency
poetry add <package-name>

# Add a development dependency
poetry add --group dev <package-name>

# Update dependencies
poetry update
```

### Testing

```bash
# Run all tests (once tests are implemented)
poetry run pytest

# Run tests with coverage
poetry run pytest --cov=src --cov-report=html

# Run specific test file
poetry run pytest tests/test_<module>.py

# Run tests matching a pattern
poetry run pytest -k "test_pattern"
```

### Code Quality

```bash
# Format code with black (once configured)
poetry run black src/ tests/

# Lint with ruff (once configured)
poetry run ruff check src/ tests/

# Type checking with mypy (once configured)
poetry run mypy src/
```

### Training & Experiments

```bash
# Train an agent (once training pipeline is implemented)
poetry run python -m src.training.train --config configs/<config-file>.yaml

# Launch Jupyter Lab for analysis
poetry run jupyter lab

# Start TensorBoard
poetry run tensorboard --logdir=data/logs
```

## Project Architecture

### High-Level Structure

The project follows a standard ML research structure with clear separation of concerns:

- **`src/agents/`**: RL algorithm implementations (DQN, PPO). These wrap Stable-Baselines3 implementations with custom configurations.
- **`src/environments/`**: Gym environment wrappers that preprocess Mario game states (frame stacking, grayscale conversion, normalization).
- **`src/models/`**: Neural network architectures (CNN-based Q-networks, policy networks).
- **`src/preprocessing/`**: Frame preprocessing pipeline and feature extraction utilities.
- **`src/training/`**: Training orchestration, callbacks, checkpointing logic.
- **`src/utils/`**: Database connections, logging helpers, config loaders.

### Data Flow

1. **Environment** ‚Üí Raw game frames (240x256 RGB)
2. **Preprocessing** ‚Üí Grayscale, resize, normalize, stack frames
3. **Agent/Model** ‚Üí Neural network processes state, outputs action
4. **Training Loop** ‚Üí Collects experience, updates model, logs metrics
5. **Storage** ‚Üí PostgreSQL (experiment metadata), MLflow (models), W&B (live metrics)

### Key Design Patterns

- **Configuration-Driven**: Hyperparameters and experiment settings stored in YAML files (`configs/`).
- **Database-Backed Experiments**: All training runs logged to PostgreSQL with metadata for reproducibility.
- **Multi-Tool Tracking**: MLflow for model versioning, Weights & Biases for real-time monitoring, local storage for checkpoints.
- **Modular Components**: Environment wrappers, preprocessing, and agents are independently testable.

## Technology Stack

### Core ML

- **PyTorch**: Neural network framework
- **Stable-Baselines3**: Production RL algorithms
- **Gymnasium**: RL environment interface
- **gym-super-mario-bros**: NES Mario environment

### Infrastructure

- **Poetry**: Python dependency management (Python 3.14+)
- **PostgreSQL**: Experiment metadata and results
- **MLflow**: Model registry and versioning
- **Weights & Biases**: Real-time training metrics
- **DVC**: Data version control (planned)

### Development

- **pytest**: Testing framework
- **black**: Code formatter
- **ruff**: Linter
- **mypy**: Type checker
- **Docker**: Containerization for training

### Analysis

- **Jupyter Lab**: Exploratory notebooks
- **Plotly/Seaborn**: Visualization
- **TensorBoard**: Training metrics

## Important Workflow Notes

### When Adding Dependencies

Dependencies must be added through Poetry (`poetry add <package>`), not pip. The `pyproject.toml` uses Poetry's build system.

### Database Schema

When modifying the database schema (in `database/`), create migration scripts rather than directly altering tables to maintain experiment history integrity.

### Experiment Configuration

All training experiments should have corresponding YAML configs in `configs/` with:

- Algorithm hyperparameters
- Network architecture specs
- Training duration and checkpointing settings
- Environment preprocessing parameters

### Model Checkpoints

Saved models go in `models/` with naming convention: `{algorithm}_{timestamp}_{git_commit}.pth`

### Documentation

This project uses Obsidian for documentation (`docs/`). When making significant architectural decisions or encountering learning insights, add notes to:

- `docs/daily/` for daily progress logs
- `docs/ProjectDocumentation.md` for architectural updates

### Phase-Based Development

The project follows structured implementation phases (see ProjectDocumentation.md):

- ‚úÖ Phase 1: Environment Setup (Complete - Dec 26-29, 2025)
- **‚Üí Phase 2: Baseline Agent (Current - Starting Jan 2026)**
- Phase 3: Simple RL Algorithm
- Phase 4: Advanced Techniques
- Phase 5: Production & Analysis
- Phase 6: Extensions

**Phase 1 Achievements:**
- PostgreSQL database with 4-table relational schema
- Weights & Biases cloud experiment tracking
- Pre-commit hooks for automated code quality
- Complete project infrastructure and documentation system

Consult `docs/ProjectDocumentation.md` for detailed phase requirements, timelines, and current progress.

## Future Cybersecurity Applications

Skills developed here will transfer to:

- ML-Enhanced Suricata Rules (network traffic anomaly detection)
- Intelligent Incident Management (alert correlation)
- Threat Intelligence (automated IOC extraction)

When implementing features, consider how they might apply to security contexts (e.g., preprocessing pipelines for network packet data, experiment tracking for threat detection model tuning).
